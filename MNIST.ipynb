{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhes23/deep_learning/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAdxJ3ukrXg4"
      },
      "outputs": [],
      "source": [
        "! pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKJei9hr3MlP"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from enum import Enum\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.graph_objs as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_H7qxjT2_r8"
      },
      "source": [
        "# Importing and plotting samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjBCjYrvwVre"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, train_size=0.8)\n",
        "\n",
        "labels = [\"T-shirt/top\",\"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "wandb.login(key = \"39e4e3cb3e968e93d443865e4c84210177e9ada5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YE0Sg9qA0aE"
      },
      "outputs": [],
      "source": [
        "wandb.init(project = \"CS6910_Assignment-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKKTyjeynEAH"
      },
      "outputs": [],
      "source": [
        "no_of_items = 10\n",
        "\n",
        "fig, axes = plt.subplots(nrows = 1, ncols=10, figsize = (28, 28))\n",
        "sample_images = []\n",
        "for item in range(no_of_items):\n",
        "  index = 0\n",
        "  while(y_train[index] != item): #searching for the first occurance of the element\n",
        "    index = index+1\n",
        "  \n",
        "  sample_images.append(wandb.Image(x_train[index], caption= labels[item]))\n",
        "\n",
        "  ax = axes[item]\n",
        "  ax.imshow(x_train[index], cmap='gray_r')\n",
        "  ax.set_title(labels[item])\n",
        "\n",
        "wandb.log({\"Sample images from each class\": sample_images})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfO1a_Bl3F7U"
      },
      "source": [
        "# Reshaping the data for nerual network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX-O8TPP3FTm"
      },
      "outputs": [],
      "source": [
        "no_of_pixels = x_train[0].size\n",
        "x_train = x_train.reshape(-1, no_of_pixels)/255\n",
        "x_test = x_test.reshape(-1, no_of_pixels)/255\n",
        "x_validation = x_validation.reshape(-1, no_of_pixels)/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCrUeXGzewLj"
      },
      "source": [
        "# Some Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMEmeqnafDw5"
      },
      "outputs": [],
      "source": [
        "class ActivationFunction(Enum):\n",
        "  SIGMOID = \"sigmoid\"\n",
        "  RELU = \"relu\"\n",
        "  TAN_H = \"tanh\"\n",
        "\n",
        "class InitializationMethod(Enum):\n",
        "  UNIFORM_RANDOM = \"uniform_rand\"\n",
        "  UNIFORM_XAVIER = \"uniform_xav\"\n",
        "  GAUSSIAN_XAVIER = \"gaussian_xav\"\n",
        "\n",
        "class OutputFunction(Enum):\n",
        "  SOFTMAX = \"softmax\"\n",
        "\n",
        "class OptimizationAlgorithm(Enum):\n",
        "  GD = \"gd\"\n",
        "  SGD = \"sgd\"\n",
        "  MINI_BATCH = \"mini_batch\"\n",
        "  MOMENTUM_GD = \"momentum_gd\"\n",
        "  NAG = \"nag\"\n",
        "  RMS_PROP = \"rms\"\n",
        "  ADAM = \"adam\"\n",
        "  NADAM = \"nadam\"\n",
        "\n",
        "class ErrorCalculationMethod(Enum):\n",
        "  CROSS_ENTROPY = \"cross_entropy\"\n",
        "  MEAN_SQUARE_ERROR = \"mse\"\n",
        "\n",
        "\n",
        "def activation_function(a, func):\n",
        "  \"\"\"\n",
        "  Calculates post activation values from pre-activation values, functions implemented:\n",
        "  * Sigmoid\n",
        "  * ReLU\n",
        "  * Tan h\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  a: ndarray, pre-activation values\n",
        "  func: Enum describing the activation function type\n",
        "\n",
        "  Retruns:\n",
        "  -------\n",
        "  Post activation values in ndarray of the same dimention\n",
        "  \"\"\"\n",
        "  if(func == ActivationFunction.SIGMOID):   \n",
        "    # clipping_limit = 400\n",
        "    # return 1.0 / (1.0 + np.exp(-np.clip(a,-clipping_limit,clipping_limit)))\n",
        "    new = a.copy()\n",
        "    new[a<0] = np.exp(a[a<0])/(1.0 + np.exp(a[a<0]))\n",
        "    new[a>=0] = 1/(1+np.exp(-a[a>=0]))\n",
        "    return new\n",
        "\n",
        "  if(func == ActivationFunction.RELU):\n",
        "    return np.maximum(0,a)\n",
        "\n",
        "  if(func == ActivationFunction.TAN_H):\n",
        "    return np.tanh(a)\n",
        "\n",
        "\n",
        "def df_activation_function(a, func):\n",
        "  \"\"\"\n",
        "  Calculates the derivative of the activation function\n",
        "  \n",
        "  Parameters:\n",
        "  -------\n",
        "  a: ndarray, pre-activation values\n",
        "  func: Enum describing the activation function type\n",
        "  \n",
        "  \"\"\"\n",
        "  if(func == ActivationFunction.SIGMOID): \n",
        "    return activation_function(a, func) * (1 - activation_function(a, func))\n",
        "\n",
        "  if(func == ActivationFunction.RELU):\n",
        "    result = a.copy()\n",
        "    result[result>=0] = 1\n",
        "    result[result<0] = 0\n",
        "    return result\n",
        "  \n",
        "  if(func == ActivationFunction.TAN_H):\n",
        "    return 1 - np.square(activation_function(a, func))\n",
        "\n",
        "\n",
        "\n",
        "def output_function(a, func):\n",
        "  \"\"\"\n",
        "  Given the pre-activation values, post activation values of the output layer\n",
        "  \"\"\"\n",
        "  if(func == OutputFunction.SOFTMAX): \n",
        "    # a = a - np.max(a)\n",
        "    # return np.exp(a) / np.sum(np.exp(a), axis=1, keepdims=True)\n",
        "    num = np.exp(a - ((np.ones(shape=(a.shape[1],a.shape[0]))* (np.max(a,axis=1))).transpose()))\n",
        "    den = ((np.ones(shape=(a.shape[1],a.shape[0]))* (1/(np.sum(num,axis=1)))).transpose())\n",
        "    return np.multiply(num,den)\n",
        "\n",
        "\n",
        "def calc_total_error(predicted_distribution, true_label, method, weight_decay_for_l2_reg, weights):\n",
        "  \"\"\"Calculates the total error based on the error calculation method\n",
        "  \n",
        "  Params:\n",
        "  --------\n",
        "  predicted_distribution:\n",
        "    ndarray containing the predicted probability distribution for each input\n",
        "  true label:\n",
        "    ndarray containing the true label for each inputs\n",
        "  method:\n",
        "    Enum describing the type of error calculation method used\n",
        "  weight_decay_for_l2_reg:\n",
        "    weight decay used \n",
        "  weights: \n",
        "    ndarray containings weights at each layer \n",
        "  \"\"\"\n",
        "  rows = np.arange(true_label.shape[0]) #setting row number from 0 to length(true label)\n",
        "  cols = true_label\n",
        "  L = 0.0\n",
        "  if(method == ErrorCalculationMethod.CROSS_ENTROPY):\n",
        "    predicted_distribution = predicted_distribution[rows,cols]\n",
        "    predicted_distribution[predicted_distribution == 0] = 1e-6  #setting 0 values to very small value, so we dont get inf \n",
        "    L = sum(-np.log(predicted_distribution)) \n",
        "\n",
        "  if(method == ErrorCalculationMethod.MEAN_SQUARE_ERROR):\n",
        "    predicted_distribution[rows,cols] -= 1\n",
        "    L = np.sum(predicted_distribution ** 2)\n",
        "  \n",
        "  #starting from index:1 because we are not using index:0\n",
        "  val = 0\n",
        "  for i in range(1, len(weights)):\n",
        "    val += np.sum(weights[i] ** 2)\n",
        "\n",
        "  return L + (weight_decay_for_l2_reg / 2)*val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8laK_twVri"
      },
      "source": [
        "# Main Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Wcr7PFwVrj"
      },
      "outputs": [],
      "source": [
        "class Classification:\n",
        "  weight = []\n",
        "  bias = []\n",
        "  \n",
        "  #Best Hyperparameters are initialized as default parameters\n",
        "  def __init__(self, _no_of_class = no_of_items, _hidden_layer = [128, 128], _input_layer = no_of_pixels, _max_epoch=15, _optimization_algorithm=OptimizationAlgorithm.ADAM, _activation_fun=ActivationFunction.TAN_H, _initialization_method=InitializationMethod.UNIFORM_XAVIER, _output_function = OutputFunction.SOFTMAX, _error_calculation=ErrorCalculationMethod.CROSS_ENTROPY, _learning_rate = 0.0001, _batch_size = 16, _momentum = 0.9, _decay_rate_for_update = 0.99, _weight_decay_l2_reg = 0.0005) -> None:\n",
        "    self.no_of_class = _no_of_class\n",
        "    self.hidden_layer = _hidden_layer\n",
        "    self.activation_func = _activation_fun\n",
        "    self.max_epoch = _max_epoch\n",
        "    self.input_layer = _input_layer\n",
        "    self.output_func = _output_function\n",
        "    self.learning_rate = _learning_rate\n",
        "    self.initialization_method = _initialization_method\n",
        "    self.error_calculation = _error_calculation\n",
        "    self.optimization_algorithm = _optimization_algorithm\n",
        "    self.batch_size = _batch_size\n",
        "    self.momentum = _momentum\n",
        "    self.decay_rate_for_update = _decay_rate_for_update\n",
        "    self.weight_decay_l2_reg = _weight_decay_l2_reg\n",
        "\n",
        "    #inferred values (for easier calculation)\n",
        "    self.layer = [_input_layer]\n",
        "    self.layer = self.layer + _hidden_layer\n",
        "    self.layer.append(_no_of_class)    \n",
        "    self.L = len(self.layer) - 1  #number of layers excluding the input layer (L value used in the lectures)\n",
        "\n",
        "\n",
        "  def init_weight_and_bias(self):\n",
        "    #going to use 1-based indexing (as tought in the class)\n",
        "    #so adding some random matrix in 0-th index\n",
        "    w = [np.random.rand(1,1)]\n",
        "    b = [np.random.rand(1,1)]\n",
        "    if(self.initialization_method == InitializationMethod.UNIFORM_RANDOM): \n",
        "      low = -1\n",
        "      high = 1\n",
        "      i = 1\n",
        "      while i < len(self.layer):\n",
        "        w.append(np.random.uniform(low, high, size=(self.layer[i], self.layer[i-1])))\n",
        "        b.append(np.zeros(self.layer[i]))\n",
        "        i +=1\n",
        "    \n",
        "    if(self.initialization_method == InitializationMethod.UNIFORM_XAVIER): \n",
        "      for i in range(1, len(self.layer)):\n",
        "        inputs = self.layer[i-1]\n",
        "        outputs = self.layer[i]\n",
        "        # x = math.sqrt(6/ inputs+outputs)\n",
        "        x = math.sqrt(1/inputs)\n",
        "        w.append(np.random.uniform(low=-x, high=x, size=(self.layer[i], self.layer[i-1])))\n",
        "        b.append(np.zeros(self.layer[i]))\n",
        "\n",
        "    if(self.initialization_method == InitializationMethod.GAUSSIAN_XAVIER): \n",
        "      mu = 0.0\n",
        "      for i in range(1, len(self.layer)):\n",
        "        inputs = self.layer[i-1]\n",
        "        outputs = self.layer[i]\n",
        "        # sigma = math.sqrt(6 / inputs+outputs)\n",
        "        sigma = math.sqrt(1/ inputs)\n",
        "        w.append(np.random.normal(mu, sigma, size=(self.layer[i], self.layer[i-1])))\n",
        "        b.append(np.zeros(self.layer[i]))\n",
        "    self.weight = np.array(w, dtype=object)\n",
        "    self.bias = np.array(b, dtype=object)\n",
        "\n",
        "\n",
        "  def init_prev_moments(self):\n",
        "    \"\"\"\n",
        "    Returns the previous moments for both weight and bias (ie zeros) with same dimention as weights and biases\n",
        "\n",
        "    Returns:\n",
        "    ------\n",
        "    prev_moment_w: ndarray, dtype=object\n",
        "      All the values are initialized with zero, with same structure as weight matrix of the network\n",
        "    prev_moment_b: ndarray, dtype=object\n",
        "      All the values are initialized with zero, with same structure as bias matrix of the network\n",
        "    \"\"\"\n",
        "    #going to use 1-based indexing (as tought in the class)\n",
        "    #so adding some random matrix in 0-th index\n",
        "    prev_moment_w = [np.random.rand(1,1)]\n",
        "    prev_moment_b = [np.random.rand(1,1)]\n",
        "    for i in range(1, len(self.layer)):\n",
        "      prev_moment_w.append(np.zeros(shape=(self.layer[i], self.layer[i-1])))\n",
        "      prev_moment_b.append(np.zeros(self.layer[i]))\n",
        "\n",
        "    return np.array(prev_moment_w, dtype=object), np.array(prev_moment_b, dtype=object)\n",
        "\n",
        "  def forward_propogation(self, input_images):\n",
        "    #for using 1 based indexing, adding some random matrix in 0-th index\n",
        "    a = [np.random.rand(1,1)]\n",
        "    h = [input_images]\n",
        "\n",
        "    for i in range(1, self.L):\n",
        "      a.append(self.bias[i] + np.dot(h[i-1], self.weight[i].T))\n",
        "      h.append(activation_function(a[i], self.activation_func))\n",
        "\n",
        "    a.append(self.bias[self.L] + np.dot(h[self.L-1], self.weight[self.L].T))\n",
        "    h.append(output_function(a[-1], self.output_func))\n",
        "    return a, h\n",
        "\n",
        "\n",
        "  def backward_propogation(self, a, h, true_label, weight = None, use_custom_weight_matrix = False):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    ------\n",
        "    a: list of ndarray\n",
        "      pre activation values of the layers of the network (for the data points the in the batch)\n",
        "    h: list of ndarray\n",
        "      post activation values of the layers of the network (for the data points the in the batch)\n",
        "    true_label: ndarray\n",
        "      true label values of each of the data points in the batch\n",
        "    weight: ndarray\n",
        "      By default weight = weights populated in the network\n",
        "      But we can pass a custom weight matrix to perfom back propogation\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    del_w: ndarray\n",
        "    del_b: ndarray\n",
        "    \"\"\"\n",
        "    if(use_custom_weight_matrix == False):\n",
        "      weight = self.weight\n",
        "    \n",
        "    del_a = [None] * (self.L+1)\n",
        "    del_h = [None] * (self.L+1)\n",
        "    del_w = [None] * (self.L+1)\n",
        "    del_b = [None] * (self.L+1)\n",
        "    \n",
        "    #computing del_a_l\n",
        "    del_a[-1] = h[-1].copy()\n",
        "    row_ind = np.arange(true_label.shape[0]) #creating numbers 0 to batch size (for row indices)\n",
        "    del_a[-1][row_ind,true_label] -= 1\n",
        "\n",
        "    for k in range(self.L, 0, -1):\n",
        "      #computing gradients w.r.t parameters\n",
        "      del_w[k] = np.dot(del_a[k].T, h[k-1])\n",
        "      del_b[k] = np.sum(del_a[k], axis=0)\n",
        "\n",
        "      #computing gradients w.r.t layer below (post-activation)\n",
        "      del_h[k-1] = np.dot(del_a[k],weight[k])\n",
        "\n",
        "      #computing gradients w.r.t layer below (pre-activation)\n",
        "      del_a[k-1] = del_h[k-1] * df_activation_function(a[k-1], self.activation_func)\n",
        "    \n",
        "    #setting the 0-th index to some random array of (1,1)\n",
        "    #so that it won't cause dimention mismatch\n",
        "    del_w[0] = np.random.rand(1,1)\n",
        "    del_b[0] = np.random.rand(1,1)\n",
        "    return np.array(del_w, dtype=object), np.array(del_b, dtype=object)\n",
        "\n",
        "\n",
        "  def update_mini_batch(self, del_w, del_b):\n",
        "    self.weight = self.weight - (self.learning_rate * del_w) - (self.learning_rate * self.weight_decay_l2_reg * self.weight)\n",
        "    self.bias = self.bias - (self.learning_rate * del_b) - (self.learning_rate * self.weight_decay_l2_reg * self.bias)\n",
        "\n",
        "\n",
        "  def update_momentum_gd(self, prev_moment_w, prev_moment_b, del_w, del_b):\n",
        "    prev_moment_w = self.momentum * prev_moment_w + (1 - self.momentum) * del_w\n",
        "    prev_moment_b = self.momentum * prev_moment_b + (1 - self.momentum) * del_b\n",
        "\n",
        "    self.weight = self.weight - (self.learning_rate * prev_moment_w) - (self.learning_rate * self.weight_decay_l2_reg * self.weight)\n",
        "    self.bias = self.bias - (self.learning_rate * prev_moment_b) - (self.learning_rate * self.weight_decay_l2_reg * self.bias)\n",
        "    \n",
        "    return prev_moment_w, prev_moment_b\n",
        "\n",
        "\n",
        "  def update_nag(self, prev_moment_w, prev_moment_b, del_w_lookahead, del_b):\n",
        "    prev_moment_w = self.momentum * prev_moment_w + (1 - self.momentum) * del_w_lookahead\n",
        "    prev_moment_b = self.momentum * prev_moment_b + (1 - self.momentum) * del_b\n",
        "\n",
        "    self.weight = self.weight - (self.learning_rate * prev_moment_w) - (self.learning_rate * self.weight_decay_l2_reg * self.weight)\n",
        "    self.bias = self.bias - (self.learning_rate * prev_moment_b) - (self.learning_rate * self.weight_decay_l2_reg * self.bias)\n",
        "    return prev_moment_w, prev_moment_b\n",
        "  \n",
        "\n",
        "  def update_rms_prop(self, prev_update_w, prev_update_b, del_w, del_b):\n",
        "    small_const = 1e-8\n",
        "    prev_update_w = self.decay_rate_for_update * prev_update_w + (1 - self.decay_rate_for_update) * np.square(del_w)\n",
        "    prev_update_b = self.decay_rate_for_update * prev_update_b + (1 - self.decay_rate_for_update) * np.square(del_b)\n",
        "\n",
        "    new_lr_w = self.learning_rate / (prev_update_w**0.5 + small_const)\n",
        "    new_lr_b = self.learning_rate / (prev_update_b**0.5 + small_const)\n",
        "\n",
        "    self.weight = self.weight - (new_lr_w * del_w) - (self.learning_rate * self.weight_decay_l2_reg * self.weight)\n",
        "    self.bias = self.bias - (new_lr_b * del_b) - (self.learning_rate * self.weight_decay_l2_reg * self.bias)\n",
        "    return prev_update_w, prev_update_b\n",
        "\n",
        "\n",
        "  def update_adam(self, x_train, y_train, prev_moment_w, prev_moment_b, prev_update_w, prev_update_b, del_w, del_b):\n",
        "    small_const = 1e-8\n",
        "    prev_moment_w = self.momentum * prev_moment_w + (1 - self.momentum) * del_w\n",
        "    prev_w_hat = prev_moment_w / (1 - self.momentum)\n",
        "    prev_moment_b = self.momentum * prev_moment_b + (1 - self.momentum) * del_b\n",
        "    prev_b_hat = prev_moment_b / (1 - self.momentum)\n",
        "\n",
        "    prev_update_w = self.momentum * prev_update_w + (1 - self.momentum) * np.square(del_w)\n",
        "    prev_update_m_hat = prev_update_w / (1 - self.momentum)\n",
        "    prev_update_b = self.momentum * prev_update_b + (1 - self.momentum) * np.square(del_b)\n",
        "    prev_update_b_hat = prev_update_b / (1 - self.momentum)\n",
        "\n",
        "    new_lr_w = self.learning_rate / (prev_update_m_hat**0.5 + small_const)\n",
        "    new_lr_b = self.learning_rate / (prev_update_b_hat**0.5 + small_const)\n",
        "\n",
        "    self.weight = self.weight - (new_lr_w * prev_w_hat) - (self.learning_rate * self.weight_decay_l2_reg * self.weight)\n",
        "    self.bias = self.bias - (new_lr_b * prev_b_hat) - (self.learning_rate * self.weight_decay_l2_reg * self.bias)\n",
        "\n",
        "    return prev_moment_w, prev_moment_b, prev_update_w, prev_update_b\n",
        "\n",
        "\n",
        "  def update_nadam(self, x_train, y_train, prev_moment_w, prev_moment_b, prev_update_w, prev_update_b, del_w, del_b):\n",
        "    small_const = 1e-8\n",
        "    prev_moment_w = self.momentum * prev_moment_w + (1 - self.momentum) * del_w\n",
        "    prev_w_hat = prev_moment_w / (1 - self.momentum)\n",
        "    prev_moment_b = self.momentum * prev_moment_b + (1 - self.momentum) * del_b\n",
        "    prev_b_hat = prev_moment_b / (1 - self.momentum)\n",
        "\n",
        "    prev_update_w = self.decay_rate_for_update * prev_update_w + (1 - self.decay_rate_for_update) * np.square(del_w)\n",
        "    prev_update_m_hat = prev_update_w / (1 - self.decay_rate_for_update)\n",
        "    prev_update_b = self.decay_rate_for_update * prev_update_b + (1 - self.decay_rate_for_update) * np.square(del_b)\n",
        "    prev_update_b_hat = prev_update_b / (1 - self.decay_rate_for_update)\n",
        "\n",
        "    new_lr_w = self.learning_rate / (prev_update_m_hat**0.5 + small_const)\n",
        "    new_lr_b = self.learning_rate / (prev_update_b_hat**0.5 + small_const)\n",
        "\n",
        "    self.weight = self.weight - (new_lr_w * (self.momentum * prev_w_hat + (((1/self.momentum) * del_w) / (1-self.momentum)))) - (self.learning_rate * self.weight_decay_l2_reg * self.weight)\n",
        "    self.bias = self.bias - (new_lr_b * (self.momentum * prev_b_hat + (((1/self.momentum) * del_b) / (1-self.momentum)))) - (self.learning_rate * self.weight_decay_l2_reg * self.bias)\n",
        "\n",
        "    return prev_moment_w, prev_moment_b, prev_update_w, prev_update_b\n",
        "\n",
        "\n",
        "  def fit(self, x_train, y_train, x_validation, y_validation): \n",
        "    self.data_size = x_train.shape[0]\n",
        "    no_of_batches = self.data_size // self.batch_size\n",
        "    prev_moment_w, prev_moment_b = self.init_prev_moments()\n",
        "    prev_update_w, prev_update_b = prev_moment_w.copy(), prev_moment_b.copy()\n",
        "\n",
        "    training_error_list = []\n",
        "    validation_error_list = []\n",
        "    training_accuracy = []\n",
        "    validation_accuracy = []\n",
        "\n",
        "    for i in range(self.max_epoch):\n",
        "      err = 0\n",
        "      for j in range(no_of_batches+1): \n",
        "        begin = j * self.batch_size\n",
        "        end = begin + self.batch_size\n",
        "        if(end > self.data_size):\n",
        "          end = self.data_size\n",
        "\n",
        "        a, h = self.forward_propogation(x_train[begin:end])\n",
        "\n",
        "        #insert your algorithm which needs look ahead here:\n",
        "        #----------------------------------------------\n",
        "        if(self.optimization_algorithm == OptimizationAlgorithm.NAG):\n",
        "          w_lookahead = self.weight - self.momentum * prev_moment_w\n",
        "          del_w_lookahead, del_b = self.backward_propogation(a, h, y_train[begin:end],weight=w_lookahead, use_custom_weight_matrix=True)\n",
        "          prev_moment_w, prev_moment_b = self.update_nag(prev_moment_w, prev_moment_b, del_w_lookahead, del_b)\n",
        "\n",
        "        #inser your algorithm which doesn't need lookahead here:\n",
        "        #---------------------------------------------------\n",
        "        del_w, del_b = self.backward_propogation(a, h, y_train[begin:end])\n",
        "        if(self.optimization_algorithm == OptimizationAlgorithm.MINI_BATCH):\n",
        "          self.update_mini_batch(del_w, del_b)\n",
        "        \n",
        "        if(self.optimization_algorithm == OptimizationAlgorithm.SGD):\n",
        "          self.update_mini_batch(del_w, del_b)\n",
        "        \n",
        "        if(self.optimization_algorithm == OptimizationAlgorithm.MOMENTUM_GD):\n",
        "          prev_moment_w, prev_moment_b = self.update_momentum_gd(prev_moment_w, prev_moment_b, del_w, del_b)\n",
        "        \n",
        "        if(self.optimization_algorithm == OptimizationAlgorithm.NAG):\n",
        "          prev_moment_w, prev_moment_b = self.update_nag(prev_moment_w, prev_moment_b, del_w_lookahead, del_b)\n",
        "        \n",
        "        if(self.optimization_algorithm == OptimizationAlgorithm.RMS_PROP):\n",
        "          prev_update_w, prev_update_b = self.update_rms_prop(prev_update_w, prev_update_b, del_w, del_b)\n",
        "\n",
        "        if(self.optimization_algorithm == OptimizationAlgorithm.ADAM):\n",
        "          prev_moment_w, prev_moment_b, prev_update_w, prev_update_b = self.update_adam(x_train[begin:end], y_train[begin:end], prev_moment_w, prev_moment_b, prev_update_w, prev_update_b, del_w, del_b)\n",
        "\n",
        "        if(self.optimization_algorithm == OptimizationAlgorithm.NADAM):\n",
        "            prev_moment_w, prev_moment_b, prev_update_w, prev_update_b = self.update_nadam(x_train[begin:end], y_train[begin:end], prev_moment_w, prev_moment_b, prev_update_w, prev_update_b, del_w, del_b)\n",
        "\n",
        "        err += calc_total_error(h[-1], y_train[begin:end], self.error_calculation, self.weight_decay_l2_reg, self.weight)\n",
        "      err /= self.data_size\n",
        "      training_error_list.append(err)\n",
        "      _, h_val = self.forward_propogation(x_validation)\n",
        "      validation_error_list.append(calc_total_error(h_val[-1], y_validation, self.error_calculation, self.weight_decay_l2_reg, self.weight)/ y_validation.size)\n",
        "      acc = self.calc_accuracy(x_train, y_train)\n",
        "      training_accuracy.append(acc)\n",
        "      validation_accuracy.append(self.calc_accuracy(x_validation, y_validation))\n",
        "      print(\"Completed epoch : {} \\t Error: {} \\t Accuracy: {}\".format(i+1, err, acc))\n",
        "    return training_error_list, validation_error_list, training_accuracy, validation_accuracy\n",
        "     \n",
        "\n",
        "  def calc_accuracy(self, x_test, y_test, return_predicted_distribution = False):\n",
        "    _ , h = self.forward_propogation(x_test)\n",
        "    predicted_distribution = h[-1]\n",
        "    if(return_predicted_distribution == True):\n",
        "      return predicted_distribution\n",
        "    return np.sum(np.argmax(predicted_distribution, axis=1) == y_test) / y_test.size\n",
        "  \n",
        "  def plot_graphs(training_errors, validation_errors, training_accuracy, validation_accuracy):\n",
        "    x = np.arange(len(training_errors))\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    ax1.plot(x, training_errors, label = \"Training Error\")\n",
        "    ax1.plot(x, validation_errors, label = \"Validation Error\")\n",
        "    ax1.set_title(\"Errors\")\n",
        "    ax1.legend()\n",
        "    ax2.plot(x, training_accuracy, label = \"Training Accuracy\")\n",
        "    ax2.plot(x, validation_accuracy, label = \"Validation Accuracy\")\n",
        "    ax2.set_title(\"Accuracy\")\n",
        "    ax2.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ35jZd2e7Jr"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMwVEtk2rXhA"
      },
      "outputs": [],
      "source": [
        "#@title Sweep {vertical-output:true}\n",
        "sweep_config = {\n",
        "    \"method\": 'random',\n",
        "    \"metric\": {\n",
        "    'name': 'accuracy',\n",
        "    'goal': 'maximize'\n",
        "    },\n",
        "    'parameters' :{\n",
        "        \"hidden_layers\": {\"values\":[2,3,4,5]},\n",
        "        \"neurons_per_hidden_layer\": {\"values\": [32,64,128]},\n",
        "        \"learning_rate\": {\"values\":[1e-3,1e-4]},\n",
        "        \"max_epoch\": {\"values\":[5,10,15]},\n",
        "        \"batch_size\": {\"values\":[16,32,64]},  #todo: check if these values can take enum\n",
        "        \"activation_function\" : {\"values\" : [\"sigmoid\", \"relu\", \"tanh\"]},\n",
        "        \"optimization_algorithm\": {\"values\":[\"mini_batch\", \"momentum_gd\", \"nag\", \"rms\", \"adam\", \"nadam\"]},\n",
        "        \"initialization_method\" : {\"values\" : [\"uniform_rand\", \"uniform_xav\"]},\n",
        "        \"weight_decay\" : {\"values\":[0, 0.0005, 0.05, 0.5]}\n",
        "    }\n",
        "}\n",
        "\n",
        "def tune_nn():\n",
        "    \"\"\"A utility function for performing the sweep\"\"\"\n",
        "    wandb.init()\n",
        "    name = \"{}_{}_hl_{}_with_{}_neurons_lr_{}_batch_{}_init_{}_l2_{}\".format(\n",
        "        wandb.config.optimization_algorithm,\n",
        "        wandb.config.activation_function,\n",
        "        wandb.config.hidden_layers,\n",
        "        wandb.config.neurons_per_hidden_layer,\n",
        "        wandb.config.learning_rate,\n",
        "        wandb.config.batch_size,\n",
        "        wandb.config.initialization_method,\n",
        "        wandb.config.weight_decay\n",
        "    )\n",
        "    wandb.run.name = name\n",
        "    no_of_hidden = wandb.config.hidden_layers \n",
        "    layer_size = wandb.config.neurons_per_hidden_layer\n",
        "    hidden_layer = [layer_size] * no_of_hidden\n",
        "\n",
        "    model = Classification(_no_of_class=10,\n",
        "                           _hidden_layer = hidden_layer,\n",
        "                           _input_layer = 784,\n",
        "                           _max_epoch = wandb.config.max_epoch,\n",
        "                           _activation_fun = ActivationFunction(wandb.config.activation_function),\n",
        "                           _initialization_method = InitializationMethod(wandb.config.initialization_method),\n",
        "                           _learning_rate = wandb.config.learning_rate,\n",
        "                           _batch_size = wandb.config.batch_size,\n",
        "                           _optimization_algorithm = OptimizationAlgorithm(wandb.config.optimization_algorithm),\n",
        "                           _weight_decay_l2_reg = wandb.config.weight_decay)\n",
        "\n",
        "    model.init_weight_and_bias()\n",
        "    tr_err, val_err, tr_acc, val_acc = model.fit(x_train, y_train, x_validation, y_validation)\n",
        "\n",
        "    for i in range(len(tr_err)):\n",
        "      wandb.log({\"tr_err\":tr_err[i],\n",
        "                 \"tr_acc\" : tr_acc[i],\n",
        "                 \"val_err\" : val_err[i],\n",
        "                 \"val_acc\" : val_acc[i],\n",
        "                 \"epoch\":(i+1)})\n",
        "      \n",
        "    wandb.log({\"accuracy\": val_acc[-1]})\n",
        "\n",
        "\n",
        "sweep_id=wandb.sweep(sweep_config,project=\"CS6910_Assignment-1\")\n",
        "wandb.agent(sweep_id,function=tune_nn,count=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H039VkOofi3X"
      },
      "outputs": [],
      "source": [
        "model_best = Classification(_max_epoch = 2) #TODO: remove all params to for best model\n",
        "model_best.init_weight_and_bias()\n",
        "tr_err, val_err, tr_acc, val_acc = model_best.fit(x_train, y_train, x_validation, y_validation)\n",
        "model_best.plot_graphs(tr_err, val_err, tr_acc, val_acc)\n",
        "predicted_dist = model_best.calc_accuracy(x_test, y_test, return_predicted_distribution=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfleoyYHA0aP"
      },
      "outputs": [],
      "source": [
        "model_best.calc_accuracy(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBOo4uH6A0aQ"
      },
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(y_true=y_test, y_pred=predicted_dist.argmax(axis=1))\n",
        "\n",
        "heatmap = go.Heatmap(z=conf_matrix, x=labels, y=labels)\n",
        "layout = go.Layout(title='Confusion Matrix')\n",
        "fig = go.Figure(data=[heatmap], layout=layout)\n",
        "\n",
        "wandb.init(project='CS6910_Assignment-1', name='confusion-matrix-plot')\n",
        "wandb.log({'confusion_matrix': fig})\n",
        "wandb.finish()\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cN7UBCKw4r7"
      },
      "outputs": [],
      "source": [
        "no_of_hidden = 4\n",
        "layer_size = 64\n",
        "hidden_layer = [layer_size] * no_of_hidden\n",
        "print(hidden_layer)\n",
        "model = Classification( _max_epoch = 10,\n",
        "                        _activation_fun = ActivationFunction(\"relu\"),\n",
        "                        _initialization_method = InitializationMethod(\"uniform_xav\"),\n",
        "                        _learning_rate = 0.0001,\n",
        "                        _batch_size = 64,\n",
        "                        _optimization_algorithm = OptimizationAlgorithm(\"nag\"))\n",
        "model.init_weight_and_bias()\n",
        "tr_err, val_err, tr_acc, val_acc = model.fit(x_train, y_train, x_validation, y_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooea0AoWB657"
      },
      "outputs": [],
      "source": [
        "#@title Sigmoid Mini GD Large data{vertical-output:true}\n",
        "\n",
        "model = Classification(_no_of_class=10, \n",
        "                       _hidden_layer=[32,32], \n",
        "                       _input_layer=784,\n",
        "                       _initialization_method=InitializationMethod(\"uniform_xav\"),\n",
        "                       _learning_rate=0.001,\n",
        "                       _activation_fun=ActivationFunction(\"tanh\"), \n",
        "                       _optimization_algorithm=OptimizationAlgorithm(\"momentum_gd\"),\n",
        "                       _batch_size = 32,\n",
        "                       _max_epoch=10)\n",
        "\n",
        "model.init_weight_and_bias()\n",
        "tr_err, val_err, tr_acc, val_acc = model.fit(x_train, y_train, x_validation, y_validation)\n",
        "model.plot_graphs(tr_err, val_err, tr_acc, val_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-POzId5A0aT"
      },
      "outputs": [],
      "source": [
        "wandb.init(project='CS6910_Assignment-1', name='momentum-tanh')\n",
        "predictions = model.calc_accuracy(x_test, y_test, return_predicted_distribution=True)\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
        "                    y_true=y_test, preds=np.argmax(predictions, axis=1),\n",
        "                    class_names=labels)})\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j55ltnkGtYx9"
      },
      "outputs": [],
      "source": [
        "model = Classification(_no_of_class=10, \n",
        "                       _hidden_layer=[32, 32], \n",
        "                       _input_layer=784,\n",
        "                       _initialization_method=InitializationMethod(\"uniform_xav\"),\n",
        "                       _learning_rate=0.001,\n",
        "                       _activation_fun=ActivationFunction(\"tanh\"), \n",
        "                       _optimization_algorithm=OptimizationAlgorithm(\"momentum_gd\"),\n",
        "                       _batch_size = 32,\n",
        "                       _max_epoch=10,\n",
        "                       _weight_decay_l2_reg = 0.5)\n",
        "\n",
        "model.init_weight_and_bias()\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll89Ncjdy9kK"
      },
      "outputs": [],
      "source": [
        "model1 = Classification(_no_of_class=10,\n",
        "                        _hidden_layer = [32, 32],\n",
        "                        _input_layer = 784,\n",
        "                        _max_epoch = 5,\n",
        "                        _activation_fun = ActivationFunction.TAN_H,\n",
        "                        _initialization_method = InitializationMethod.UNIFORM_XAVIER,\n",
        "                        _learning_rate = 0.001,\n",
        "                        _batch_size = 32,\n",
        "                        _optimization_algorithm = OptimizationAlgorithm.ADAM)\n",
        "model1.init_weight_and_bias()\n",
        "tr_err, val_err, tr_acc, val_acc = model1.fit(x_train, y_train, x_validation, y_validation)\n",
        "model1.plot_graphs(tr_err, val_err, tr_acc, val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGxxXe2MA0aU"
      },
      "outputs": [],
      "source": [
        "wandb.init(project='CS6910_Assignment-1', name='tanh_adam')\n",
        "predictions = model1.calc_accuracy(x_test, y_test, return_predicted_distribution=True)\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
        "                    y_true=y_test, preds=predictions,\n",
        "                    class_names=labels)})\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTEDUkt2Ip12"
      },
      "outputs": [],
      "source": [
        "#@title ReLU GD Small Data { vertical-output: true}\n",
        "model1 = Classification(_no_of_class=10, \n",
        "                       _hidden_layer=[64, 64, 64], \n",
        "                       _input_layer=784,\n",
        "                       _initialization_method=InitializationMethod.UNIFORM_XAVIER,\n",
        "                       _learning_rate=0.01,\n",
        "                       _activation_fun=ActivationFunction.TAN_H, \n",
        "                       _optimization_algorithm=OptimizationAlgorithm.GD,\n",
        "                       _max_epoch=800)\n",
        "\n",
        "model1.init_weight_and_bias()\n",
        "tr_err, val_err, tr_acc, val_acc = model1.fit(x_train, y_train, x_validation, y_validation)\n",
        "model1.plot_graphs(tr_err, val_err, tr_acc, val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9XXZ0fZmEUO6"
      },
      "outputs": [],
      "source": [
        "#@title Sigmoid GD Small data{ vertical-output: true}\n",
        "model1 = Classification(_no_of_class=10, \n",
        "                       _hidden_layer=[64, 64], \n",
        "                       _input_layer=784,\n",
        "                       _initialization_method=InitializationMethod.UNIFORM_XAVIER,\n",
        "                       _learning_rate=0.01,\n",
        "                       _activation_fun=ActivationFunction.SIGMOID, \n",
        "                       _optimization_algorithm=OptimizationAlgorithm.GD,\n",
        "                       _max_epoch=2)\n",
        "model1.init_weight_and_bias()\n",
        "\n",
        "model1.init_weight_and_bias()\n",
        "tr_err, val_err, tr_acc, val_acc = model1.fit(x_train, y_train, x_validation, y_validation)\n",
        "model1.plot_graphs(tr_err, val_err, tr_acc, val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h0_GdmG_Jy_l"
      },
      "outputs": [],
      "source": [
        "#@title Sigmoid Momentum Whole data{vertical-output:true}\n",
        "model3 = Classification(_no_of_class=10, \n",
        "                       _hidden_layer=[32, 32], \n",
        "                       _input_layer=784,\n",
        "                       _initialization_method=InitializationMethod.UNIFORM_XAVIER,\n",
        "                       _learning_rate=0.01,\n",
        "                       _activation_fun=ActivationFunction.SIGMOID, \n",
        "                       _optimization_algorithm=OptimizationAlgorithm.MOMENTUM_GD,\n",
        "                       _batch_size = 50,\n",
        "                       _momentum = 0.9,\n",
        "                       _max_epoch=2)\n",
        "\n",
        "model3.init_weight_and_bias()\n",
        "tr_err, val_err, tr_acc, val_acc = model3.fit(x_train, y_train, x_validation, y_validation)\n",
        "model3.plot_graphs(tr_err, val_err, tr_acc, val_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9_QxP5GGghKL"
      },
      "outputs": [],
      "source": [
        "#@title Sigmoid NAG Whole data{vertical-output:true}\n",
        "model4 = Classification(_no_of_class=10, \n",
        "                       _hidden_layer=[32, 32], \n",
        "                       _input_layer=784,\n",
        "                       _initialization_method=InitializationMethod.UNIFORM_XAVIER,\n",
        "                       _learning_rate=0.01,\n",
        "                       _activation_fun=ActivationFunction.SIGMOID, \n",
        "                       _optimization_algorithm=OptimizationAlgorithm.NAG,\n",
        "                       _batch_size = 50,\n",
        "                       _momentum = 0.9,\n",
        "                       _max_epoch=2)\n",
        "\n",
        "model4.init_weight_and_bias()\n",
        "tr_err, val_err, tr_acc, val_acc = model4.fit(x_train, y_train, x_validation, y_validation)\n",
        "model4.plot_graphs(tr_err, val_err, tr_acc, val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cFcldgaNpKkX"
      },
      "outputs": [],
      "source": [
        "#@title Sigmoid SGD whole data{ vertical-output: true}\n",
        "model5 = Classification(_no_of_class=10, \n",
        "                       _hidden_layer=[64, 64], \n",
        "                       _input_layer=784,\n",
        "                       _initialization_method=InitializationMethod.UNIFORM_XAVIER,\n",
        "                       _learning_rate=0.01,\n",
        "                       _activation_fun=ActivationFunction.SIGMOID, \n",
        "                       _optimization_algorithm=OptimizationAlgorithm.SGD,\n",
        "                       _max_epoch=10)\n",
        "model5.init_weight_and_bias()\n",
        "\n",
        "model5.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cTln1aXTz3AW"
      },
      "outputs": [],
      "source": [
        "#@title Sigmoid RMS whole data{ vertical-output: true}\n",
        "model6 = Classification(_no_of_class=10, \n",
        "                       _hidden_layer=[32, 32], \n",
        "                       _input_layer=784,\n",
        "                       _initialization_method=InitializationMethod.UNIFORM_XAVIER,\n",
        "                       _learning_rate=0.1,\n",
        "                       _activation_fun=ActivationFunction.SIGMOID, \n",
        "                       _optimization_algorithm=OptimizationAlgorithm.RMS_PROP,\n",
        "                       _batch_size = 100,\n",
        "                       _momentum = 0.9,\n",
        "                       _max_epoch=10)\n",
        "model6.init_weight_and_bias()\n",
        "\n",
        "model6.fit(x_train, y_train)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "2b7beba6bc4576b587b472b50647070571a61f12f0cc1be023be2a084234b362"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}